---
sidebar: false
date: 2021-03-05
cover: https://cdn.jsdelivr.net/gh/hyperter96/tech-blog/docs/assets/images/background7.jpeg
tags:
 - 其它
---

# 关于研究技术的思维和规划

## 研究技术的思维

### 研究每项技术的意义

当我每次研究一项新的技术的时候，我都会思考这项技术的意义：

- 这项技术的发展背景
- 为什么要发展这项技术？有什么好处？
- 这项技术解决了什么问题？

当学完以后，可以用一句话概括/总结这项技术。这是研究技术思维最精辟的方法。举个例子：

> [!IMPORTANT] Cilium的发展背景
>
> 随着云原生的普及率越来越高，各大厂商基本上或多或少都实现了业务的 K8s 容器化，头部云计算厂商更是不用说。
> 而且随着 K8s 的 普及，当前集群逐渐呈现出以下两个特点：
>
> - 容器数量越来越多，比如：K8s 官方单集群就已经支持 150k pod
> - Pod 生命周期越来越短，Serverless 场景下甚至短至几分钟，几秒钟
>
>随着容器密度的增大，以及生命周期的变短，对原生容器网络带来的挑战也越来越大。当前 K8s Service 负载均衡 的实现现状，在 Cilium 出现之前， Service 由 `kube-proxy` 来实现，实现方式有 `userspace`，`iptables`，`ipvs` 三种模式。
>
> **Userspace**
>
> 当前模式下，`kube-proxy` 作为反向代理，监听随机端口，通过 `iptables` 规则将流量重定向到代理端口，再由 `kube-proxy` 将流量转发到 后端 pod。Service 的请求会先从用户空间进入内核 `iptables`，然后再回到用户空间，代价较大，性能较差。
>
> **Iptables**
>
> 存在的问题：
>
> - 可扩展性差。随着 service 数据达到数千个，其控制面和数据面的性能都会急剧下降。原因在于 `iptables` 控制面的接口设计中，每添加一条规则，需要遍历和修改所有的规则，其控制面性能是`O(n²)`。在数据面，规则是用链表组织的，其性能是`O(n)`。
> - LB 调度算法仅支持随机转发。
>
> **Ipvs 模式**
>
> IPVS 是专门为 LB 设计的。它用 `hash table` 管理 service，对 service 的增删查找都是`O(1)`的时间复杂度。不过 IPVS 内核模块没有 `SNAT` 功能，因此借用了 `iptables` 的 `SNAT` 功能。
IPVS 针对报文做 `DNAT` 后，将连接信息保存在 `nf_conntrack` 中，`iptables` 据此接力做 `SNAT`。该模式是目前 Kubernetes 网络性能最好的选择。但是由于 `nf_conntrack` 的复杂性，带来了很大的性能损耗。

> 所以为什么要发展Cilium？

核心原因是Cilium 位于容器编排系统和 Linux Kernel 之间，向上可以通过编排平台为容器进行网络以及相应的安全配置，向下可以通过在 Linux 内核挂载 eBPF 程序，来控制容器网络的转发行为以及安全策略执行。

> Cilium解决了什么问题？

Cilium 是基于 eBpf 的一种开源网络实现，通过在 Linux 内核动态插入强大的安全性、可见性和网络控制逻辑，提供网络互通，服务负载均衡，安全和可观测性等解决方案。简单来说可以理解为 `Kube-proxy + CNI` 网络实现。

> 一句话总结概括Cilium

Cilium 是基于 eBpf 的一种开源网络方案，原理是通过Linux 内核动态编程式交互，提供网络互通，服务负载均衡，安全和可观测性等解决方案。


### 灵魂拷问掌握程度

随着时间的推移，我们可能会忘记新学的知识，需要加强记忆才会印象深刻。从而，我们才能烂记于心。

> [!IMPORTANT] 怎样才能衡量掌握程度呢？
>
> 最好可以问问自己：
>
> - 在脑海中是否可以清晰的描绘出技术架构？
> - 是否可以一句话概括技术？
> - 这项技术在实际生活中有哪些解决方案的实践？

## 看源代码的思路

建议由浅入深，深入浅出。

- 由浅入深：找到入口文件的`main`函数，一步步深入代码的细节
- 深入浅出：了解完代码的细节，脑中应该逐渐呈现代码的流程走向图